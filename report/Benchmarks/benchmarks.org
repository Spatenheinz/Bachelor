* Results
All of the different implementations have been tested against the C# standard library eqivalent algorithms and ensures that the results produced is correct. The only exception for this ~Chacha~ as it still is a quite uncommon cipher. All implementations synthesized using Xilinx Vivado on a Zynq Zedboard, which is a low-end FPGA. For comparisons, we have chosen to include different implementations, in C, C# and OpenSLL, using ~openssl speed -evp "algorithm"~. Unfortunately we have not been able to get our hands on the board in time and we thus stand with some limitations on the benchmarking results. The reported frequency is the results synthesising the design through Xilinx Vivado. For comparisons we settled for a Raspberry pi 4B. The reason being, this having a low-end processor similar to the one on the Zedboard, a Broadcom BCM2711, Quad core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz. Because of the very promising results we further compare our results with a Intel i5-7500. All the "raw" stdout results from the benchmarks can be found in Appendix \ref{app:raw}.
#+INCLUDE: "MD5Res.org"
** SHA256
SHA naive: 2.10 Mhz\\
*Throughput*: 512*2.1Mhz=1.075GBit=134,4MB/s
*** Throughput
Since SHA256 is closely related to MD5 we would expect the results to be relatively similar, however percentage-wise the FPGA version performs relatively better as our Naive version has a throughput of 134.4 MB/s whereas OpenSSL_high is 30 MB/s faster, corresponding to ?? percent.
#+ATTR_LATEX: :align |c|c|c|c|c| :caption Benchmarking results for SHA.
|-------------------+-------+-----+-------------+--------------|
|     *Version* | Naive |  C# | OpenSSL_low | OpenSSL_high |
|-------------------+-------+-----+-------------+--------------|
| *Throughput* (MB/s) | 134.4 | 163 |        26.3 |       164.97 |
|-------------------+-------+-----+-------------+--------------|

** AES
AES naive: 25 Mhz\\
*Throughput*: 128*25Mhz=3.2GBit=400MB/s
*** Throughput
The results of AES is interesting compared to our other implementations in the sense that even the naive FPGA version is outperforming the CPU. One can notice that our naive version has a throughput of 400 MB/s which is around 4.49 times as much as OpenSLL on its peak performance and that it likewise outperforms C# and our own C-version with 5.7 and 6.2 times respectively. Even the threaded version is only half as fast as the FPGA solution. These results are quite promising in itself and clearly shows that the FPGA is very suitable for solutions as this one and in cases where large amounts of data needs to be encrypted or decrypted the FPGA is preferable over an arm processor. It is however still worth noting that this still only outperforms processors that does not have AES-NI. For instance running the same OpenSSL benchmark on an intel i7-7500 the worst case has a throughput of 788 MB/s and an optimal solution of 5.61 GB/s. Even though there is a significant price difference between the ARM and Intel processor it still hints to how AES-NI, essentially a dedicated ASIC, outperforms more general CPU solutions by an order of magnitude. One aspect which would have been interesting to measure is how well the implementation synthesized on a high-end FPGA would perform compared to cabable CPU's. An article from 2020 which compares both pipelined and non-pipelined versions of AES on FPGA's shows that even on expensive FPGA's such as the Virtex-7 family only runs between 2.06-6.34 Gbps (257.5-792.5 MB/s)\cite{FPGA_AES}. Thus a nonpipelined version will never be able to compete with an ASIC. One aspect that would have been interested exploring is how well our highlevel version would compare to a solution which has been optimized directly in one of the HDL languages. We have not been able to do this but we can hint at the fact that some of the solutions described previously performs worse than our solution despite the difference in chipset.
#+ATTR_LATEX: :align |c|c|c|c|c|c|c| :caption Benchmarking results for AES.
|-------------------+-------+----+-------+--------+-------------+--------------|
|     *Version* | Naive | C# |     C |    C_t | OpenSSL_low | OpenSSL_high |
|-------------------+-------+----+-------+--------+-------------+--------------|
| *Throughput* (MB/s) |   400 | 70 | 64.49 | 198.28 |        72.4 |        89.06 |
|-------------------+-------+----+-------+--------+-------------+--------------|
In the implementation section we described how we rejected to make a solution that was flexible in its key-size. The results hint that this have good impact on the performance. Comparing our solution to the solution presented in the SME github repository, which is more flexible in the key size, our solution outperforms this by a factor of 1.66, as it is reported to have a throughput of 1.92Gbps(240MB/s)\cite{sme}. This shows that we can tradeoff some flexibility for a significant speedup.


** ChaCha20
ChaCha naive: <5 Mhz ????\\
*Throughput*: ?
fails nets before timing, too much data for a small board.
Even though one would expect ChaCha20 to perform well, because of the simplicity in computation, our FGPA version of ChaCha20 performs poorly. As one can see from the Table our version is X times slower than the openSSL solution.
#+ATTR_LATEX: :align |c|c|c|c| :caption Benchmarking results for Chacha.
|-------------------+-------+-------------+--------------|
| *Version*           | Naive | OpenSSL_low | OpenSSL_high |
|-------------------+-------+-------------+--------------|
| *Throughput* (MB/s) | ?     |       84.03 |       306.81 |
|-------------------+-------+-------------+--------------|
 The culprit of ChaCha20's poor peformance is found in the high amount of nets. Nets is sythetic datapath in Vivado, which will be transformed into a wire when mapped to hardware. This suggests that we have too much data on the busses between the interlectual property (IP) and the register transfer level (RTL) of the design. This seems quite a reasonable argument as the input bus itself takes in 1152 bits and the output bus carries 544 bits to output the cipher.
 To have a concrete proof of this we also implemented a version which only generates the keystream. When running the keystream version through vivado we get a reported speed of 200 Mhz, which might suggest that Vivado is optimizing away some computations, and thus the results of this will be scewed. Anyways, we can from Figure\ref{} see that the naive version is taking up a lot of space on the FPGA and this will make timing even harder, especially when the net usage is too high.
