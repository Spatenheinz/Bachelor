* Results of implementation
All of the different implementations have been tested against the C# standard library eqivalent algorithms and ensures that the results produced is correct. The only exception for this ~Chacha~ as it still is a quite uncommon cipher. All implementations synthesized using Xilinx Vivado on a Zynq Zedboard, which is a low-end FPGA. For comparisons, we have chosen to include different implementations, in C, C# and OpenSLL, using ~openssl speed -evp "algorithm"~. Unfortunately we have not been able to get our hands on the board in time and we thus stand with some limitations on the benchmarking results. The reported frequency is the results synthesising the design through Xilinx Vivado. For comparisons we settled for a Raspberry pi 4B. The reason being, this having a low-end processor similar to the one on the Zedboard, a Broadcom BCM2711, Quad core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz. Because of the very promising results we further compare our results with a Intel i5-7500. All the "raw" stdout results from the benchmarks can be found in Appendix \ref{app:raw}.
** MD5
\label{sec:MD5performance}
*** Throughput
#+BEGIN_EXPORT latex
\begin{table}[!htb]
\centering
\captionsetup{width=.8\linewidth}
\begin{tabular}{c c c c c c}
\hline
Version & f$_{max}$(Mhz) & clocks & TP(MBps) & LUT & FF\\
\hline
Naive & 2.38 & b & 152.3 & 11607 & 2304\\
Proc_{4} & 9.5 &   \( 6+2 \cdot b\) & 265.9 & 10247 & 5226\\
Proc_{8} & 19 &    \(10+2 \cdot b\) & 531.7 & 10087 & 7538\\
Proc_{16} & 33.5 & \(18+2 \cdot b\) & 937.2 & 10206 & 12162\\
Proc_{32} & 65 &   \(34+2 \cdot b\) & 1816.9 & 10149 & 21347\\
Proc_{64} & 115 &  \(66+2 \cdot b\) & 3209.4 & 10350 & 39718\\
\end{tabular}
\caption[MD5-versions]%
{Performance and statistics over the different MD5 implementations. f$_{max}$ is the clockrate reported from Vivado. clocks, describes how many clock cycles it takes to calculate \texttt{b} blocks. The throughput TP is calculated as \((b_{bits}\cdot f_{max})/(clocks \cdot 8)\). LUT is the number of Look-Up Tables used in the design. FF is the reported amount of Flip Flops used. Proc$_{i}$ denotes how many i processes the 64 rounds are distributed over.}
\label{tab:MD5versions}
\end{table}
#+END_EXPORT
As can be seen in Table \ref{tab:MD5versions}, there is a monumental difference between the naive versions. Even the most simple of the pipelines have 74.6 pct. increase over the naive version and the highest performing version which calculates only a single round in each process more than 20 times faster than the naive version. This comes at a cost of a lot more Flip-Flops but with a slightly fewer LUTs. It is however quite remarkable that such performance increases is achievable without doing specific FPGA optimisations as such. Especially one thing aspect has been surprising to see. To keep track of the input block, each process simply forwards it from its input bus to its outputbus. Thus one would assume this computation would take a relatively long time compared to calculating a single round value, but this has not been the case. The reason might be because this is optimized away by Vivado.
#+BEGIN_EXPORT latex
\begin{table}[!htb]
\centering
\captionsetup{width=.8\linewidth}
\begin{tabular}{c c c c c c c c}
\hline
\textbf{Version} & Naive & Proc_{64} & C\# & C & C$_{t}$ & OpenSLL$_{low}$ & OpenSLL$_{high}$\\
\hline
\textbf{TP(MBps)} & 152 & 3210 & 287 & 154 & 256 & 42 & 293\\
 & & & 604 & 622 & 600 & 81 & 691
\end{tabular}
\caption[MD5-versions]%
{Performance comparison of the worst and best FPGA implementations and the various CPU versions. The C\# uses the \texttt{System.Security.Cryptography.MD5}, the C version and C$_t$ is our own implementations and is optimised with \texttt{-O3}. The openSSL is from \texttt{openssl speed -evp md5}. Each of the CPU implementations has two value, the first being the Pi results and the second the I5 results.}
\label{tab:MD5compare}
\end{table}
#+END_EXPORT
Comparing the implementations to the CPU versions, the naive only performs adequatly with the C version on the Pi. Likewise it seems to beat OpenSSL_low by quite a margin. The OpenSSL_low is the worst utilization of the ~openssl speed~, which happens on message sizes of 16 bytes. Compared to the worst utilization of OpenSLL this is a speedup of more than 300 pct. One should keep in mind that OpenSSL only works on inputs of 16 bytes, which is not nearly enough to fill a block and thus full blocks of data is not processed meaning there is a lot of spill. Even when running the same benchmarks on the I5 the result of 16 bytes is merely 81 MB/s. Thus to get the full utilization we should focus the attention to 256 byte blocks or higher, as the 64 byte blocks will have a round of "wasteful" computation as this block is purely padding and not part of the message size.
For all other versions the naive version performs poorly, especially on the I5 where most of the versions is around 4 times faster. The best performing pipeline on the other hand outperforms all the CPU versions by a significant amount, by atleast 4.6 times. This is a significant increase in speed which really emphasizes how well a FPGA can perform if designed correctly. One thing to keep in mind about these results however is that this is an optimal case; if the version worked on longer strings suchs performance would not be as fast.
*** Power Consumptions
From the previous section we showed that our FPGA solution could outperform not only low-end CPU's but also mid-end CPU by quite a margin. But not only are the FPGA able to achieve high throughput it also does it at a very low power consumption. Figure \ref{fig:md5_naive_power} shows the power consumption as reported by Vivado. The power consumtion of the naive version sum up to 0.016 watt without including the processing system, which is almost 11 times less than the optimized version using 0.189 watt. We can thus see one need only 11 times as much power to get a speed increase of 20 times. In any case, we can assume this to be very power efficient compared to the power used by a CPU, without having any real proof of this. We base this assumption on the fact that processing system (PS7) uses atleast 88 percent of the power.
\begin{figure}[H]
\centering
\subfloat[Naive version]{\includegraphics[width=6cm]{MD5_naive_power.png}}
\subfloat[Proc$_{64}$ version]{\includegraphics[width=6cm]{MD5_opt_power.png}}
\caption[Power consumption of MD5 designs]%
{Powerconsumption of MD5 designs}
\label{fig:md5_naive_power}
\end{figure}

** SHA256
SHA naive: 2.10 Mhz\\
*Throughput*: 512*2.1Mhz=1.075GBit=134,4MB/s
*** Throughput
Since SHA256 is closely related to MD5 we would expect the results to be relatively similar, however percentage-wise the FPGA version performs relatively better as our Naive version has a throughput of 134.4 MB/s whereas OpenSSL_high is 30 MB/s faster, corresponding to ?? percent.
#+ATTR_LATEX: :align |c|c|c|c|c| :caption Benchmarking results for SHA.
|-------------------+-------+-----+-------------+--------------|
|     *Version* | Naive |  C# | OpenSSL_low | OpenSSL_high |
|-------------------+-------+-----+-------------+--------------|
| *Throughput* (MB/s) | 134.4 | 163 |        26.3 |       164.97 |
|-------------------+-------+-----+-------------+--------------|

** AES
AES naive: 25 Mhz\\
*Throughput*: 128*25Mhz=3.2GBit=400MB/s
*** Throughput
The results of AES is interesting compared to our other implementations in the sense that even the naive FPGA version is outperforming the CPU. One can notice that our naive version has a throughput of 400 MB/s which is around 4.49 times as much as OpenSLL on its peak performance and that it likewise outperforms C# and our own C-version with 5.7 and 6.2 times respectively. Even the threaded version is only half as fast as the FPGA solution. These results are quite promising in itself and clearly shows that the FPGA is very suitable for solutions as this one and in cases where large amounts of data needs to be encrypted or decrypted the FPGA is preferable over an arm processor. It is however still worth noting that this still only outperforms processors that does not have AES-NI. For instance running the same OpenSSL benchmark on an intel i7-7500 the worst case has a throughput of 788 MB/s and an optimal solution of 5.61 GB/s. Even though there is a significant price difference between the ARM and Intel processor it still hints to how AES-NI, essentially a dedicated ASIC, outperforms more general CPU solutions by an order of magnitude. One aspect which would have been interesting to measure is how well the implementation synthesized on a high-end FPGA would perform compared to cabable CPU's. An article from 2020 which compares both pipelined and non-pipelined versions of AES on FPGA's shows that even on expensive FPGA's such as the Virtex-7 family only runs between 2.06-6.34 Gbps (257.5-792.5 MB/s)\cite{FPGA_AES}. Thus a nonpipelined version will never be able to compete with an ASIC. One aspect that would have been interested exploring is how well our highlevel version would compare to a solution which has been optimized directly in one of the HDL languages. We have not been able to do this but we can hint at the fact that some of the solutions described previously performs worse than our solution despite the difference in chipset.
#+ATTR_LATEX: :align |c|c|c|c|c|c|c| :caption Benchmarking results for AES.
|-------------------+-------+----+-------+--------+-------------+--------------|
|     *Version* | Naive | C# |     C |    C_t | OpenSSL_low | OpenSSL_high |
|-------------------+-------+----+-------+--------+-------------+--------------|
| *Throughput* (MB/s) |   400 | 70 | 64.49 | 198.28 |        72.4 |        89.06 |
|-------------------+-------+----+-------+--------+-------------+--------------|
In the implementation section we described how we rejected to make a solution that was flexible in its key-size. The results hint that this have good impact on the performance. Comparing our solution to the solution presented in the SME github repository, which is more flexible in the key size, our solution outperforms this by a factor of 1.66, as it is reported to have a throughput of 1.92Gbps(240MB/s)\cite{sme}. This shows that we can tradeoff some flexibility for a significant speedup.


** ChaCha20
ChaCha naive: <5 Mhz ????\\
*Throughput*: ?
fails nets before timing, too much data for a small board.
Even though one would expect ChaCha20 to perform well, because of the simplicity in computation, our FGPA version of ChaCha20 performs poorly. As one can see from the Table our version is X times slower than the openSSL solution.
#+ATTR_LATEX: :align |c|c|c|c| :caption Benchmarking results for Chacha.
|-------------------+-------+-------------+--------------|
| *Version*           | Naive | OpenSSL_low | OpenSSL_high |
|-------------------+-------+-------------+--------------|
| *Throughput* (MB/s) | ?     |       84.03 |       306.81 |
|-------------------+-------+-------------+--------------|
 The culprit of ChaCha20's poor peformance is found in the high amount of nets. Nets is sythetic datapath in Vivado, which will be transformed into a wire when mapped to hardware. This suggests that we have too much data on the busses between the interlectual property (IP) and the register transfer level (RTL) of the design. This seems quite a reasonable argument as the input bus itself takes in 1152 bits and the output bus carries 544 bits to output the cipher.
 To have a concrete proof of this we also implemented a version which only generates the keystream. When running the keystream version through vivado we get a reported speed of 200 Mhz, which might suggest that Vivado is optimizing away some computations, and thus the results of this will be scewed. Anyways, we can from Figure\ref{} see that the naive version is taking up a lot of space on the FPGA and this will make timing even harder, especially when the net usage is too high.
